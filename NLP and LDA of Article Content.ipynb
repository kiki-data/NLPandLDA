{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Article and Title NLP and LDA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##Scrape article and title content\n",
    "The first section uses original text to analyze for NLP features and variables. The second section uses cleaned text for latent dirichlech allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Natural Language Processing\n",
    "Adapted NLP portions from https://github.com/GarrettHoffman/digital_media_shares_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "inputData = pd.read_csv('File.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check length of final data set\n",
    "len(inputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from textstat.textstat import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "def engineer_NLP_features(inputData):\n",
    "\n",
    "    \"\"\"\n",
    "    Generate NLP fatures (related to language and sentiment)\n",
    "    for articles to be used in predicting no. of \n",
    "    reads\n",
    "\n",
    "    Arguments:\n",
    "    inputData: document contating article content data\n",
    "\n",
    "    Output:\n",
    "    Stores NLP features results for Document\n",
    "    \"\"\"\n",
    "\n",
    "    # get article headline and article content \n",
    "\n",
    "    headline = str(inputData['Title'])\n",
    "    content = str(inputData['ArticleText'])\n",
    "\n",
    "    # generate headline features\n",
    "\n",
    "    # number of words in title\n",
    "    n_tokens_title = len(headline.split())\n",
    "\n",
    "    # subjectivity\n",
    "    title_subjectivity = TextBlob(headline).subjectivity\n",
    "\n",
    "    # polarity\n",
    "    title_sentiment_polarity = TextBlob(headline).polarity\n",
    "\n",
    "    # absolute value polarirty\n",
    "    title_sentiment_abs_polarity = abs(title_sentiment_polarity)\n",
    "\n",
    "    # average word length\n",
    "    average_token_length_title = np.mean([len(w) for w \n",
    "                                          in \"\".join(c for c in headline \n",
    "                                                     if c not in string.punctuation).split()])\n",
    "\n",
    "    #generate content features\n",
    "\n",
    "    # number of words\n",
    "    n_tokens_content = len([w for w in content.split()])\n",
    "\n",
    "    # rate of unique words\n",
    "    r_unique_tokens = len(set([w.lower()\n",
    "                               for w \n",
    "                               in \"\".join(c for c in content \n",
    "                                          if c not in string.punctuation).split()]))/n_tokens_content\n",
    "\n",
    "    # rate of non-stop word\n",
    "    r_non_stop_words = len([w.lower() \n",
    "                            for w in \"\".join(c for c in content \n",
    "                                             if c not in string.punctuation).split() \n",
    "                            if w \n",
    "                            not in stop])/n_tokens_content\n",
    "\n",
    "    # rate of unique non-stop word\n",
    "    r_non_stop_unique_tokens = len(set([w.lower() \n",
    "                               for w in \"\".join(c for c in content \n",
    "                                                if c not in string.punctuation).split() \n",
    "                               if w\n",
    "                               not in stop]))/n_tokens_content\n",
    "\n",
    "    # average word length\n",
    "    average_token_length_content = np.mean([len(w) for w \n",
    "                                            in \"\".join(c for c in content\n",
    "                                                       if c not in string.punctuation).split()])\n",
    "\n",
    "    # subjectivity\n",
    "    global_subjectivity = TextBlob(content).subjectivity\n",
    "\n",
    "    # polarity\n",
    "    global_sentiment_polarity = TextBlob(content).polarity\n",
    "\n",
    "    # absolute polarity\n",
    "    global_sentiment_abs_polarity = abs(global_sentiment_polarity)\n",
    "\n",
    "    # get polarity by word\n",
    "    polarity_list = [(w, TextBlob(w).polarity) \n",
    "                     for w in \"\".join(c for c in content \n",
    "                                      if c not in string.punctuation).split()]\n",
    "\n",
    "    # global positive word rate\n",
    "    global_rate_positive_words = len([(w,p) \n",
    "                                      for (w,p) \n",
    "                                      in polarity_list \n",
    "                                      if p > 0])/len(polarity_list)\n",
    "\n",
    "    # global negative word rate\n",
    "    global_rate_negative_words = len([(w,p) \n",
    "                                      for (w,p) \n",
    "                                      in polarity_list \n",
    "                                      if p < 0])/len(polarity_list)\n",
    "\n",
    "    # positive word rate (among non-nuetral words)\n",
    "    if [(w,p) for (w,p) in polarity_list if p != 0]:\n",
    "        rate_positive_words = len([(w,p) \n",
    "                                   for (w,p) \n",
    "                                   in polarity_list \n",
    "                                   if p > 0])/len([(w,p) \n",
    "                                                   for (w,p) \n",
    "                                                   in polarity_list \n",
    "                                                   if p != 0])\n",
    "    else:\n",
    "        rate_positive_words = 0\n",
    "\n",
    "    # negative word rate (among non-nuetral words)\n",
    "    if [(w,p) for (w,p) in polarity_list if p != 0]:\n",
    "        rate_negative_words = len([(w,p) \n",
    "                                   for (w,p) \n",
    "                                   in polarity_list \n",
    "                                   if p < 0])/len([(w,p) \n",
    "                                                   for (w,p) \n",
    "                                                   in polarity_list \n",
    "                                                   if p != 0])\n",
    "\n",
    "    else:\n",
    "       rate_negative_words = 0 \n",
    "\n",
    "    # average polarity of positive words\n",
    "    if [p for (w,p) in polarity_list if p > 0]:\n",
    "        avg_positive_polarity = np.mean([p for (w,p) \n",
    "                                         in polarity_list \n",
    "                                         if p > 0])\n",
    "    else:\n",
    "        avg_positive_polarity = 0\n",
    "\n",
    "    # minimum polarity of positive words\n",
    "    if [p for (w,p) in polarity_list if p > 0]:\n",
    "        min_positive_polarity = min([p for (w,p) \n",
    "                                     in polarity_list \n",
    "                                     if p > 0])\n",
    "    else:\n",
    "        min_positive_polarity = 0\n",
    "\n",
    "    # maximum polarity of positive words\n",
    "    if [p for (w,p) in polarity_list if p > 0]:\n",
    "        max_positive_polarity = max([p for (w,p) \n",
    "                                     in polarity_list \n",
    "                                     if p > 0])\n",
    "    else: \n",
    "        max_positive_polarity = 0\n",
    "\n",
    "    # average polarity of negative words\n",
    "    if [p for (w,p) in polarity_list if p < 0]:\n",
    "        avg_negative_polarity = np.mean([p for (w,p) \n",
    "                                         in polarity_list \n",
    "                                         if p < 0])\n",
    "    else:\n",
    "        avg_negative_polarity = 0\n",
    "\n",
    "    # minimum polarity of negative words\n",
    "    if [p for (w,p) in polarity_list if p < 0]:\n",
    "        min_negative_polarity = min([p for (w,p) \n",
    "                                     in polarity_list \n",
    "                                     if p < 0])\n",
    "    else:\n",
    "        min_negative_polarity = 0\n",
    "\n",
    "    # maximum polarity of negative words\n",
    "    if [p for (w,p) in polarity_list if p < 0]:\n",
    "        max_negative_polarity = max([p for (w,p) \n",
    "                                 in polarity_list \n",
    "                                 if p < 0])\n",
    "    else:\n",
    "        max_negative_polarity = 0\n",
    "\n",
    "    # abs maximum polarity, sum of abs of max positive and abs of min negative polarity\n",
    "    max_abs_polarity = max_positive_polarity + abs(min_negative_polarity)\n",
    "\n",
    "    # Flesch Reading Ease\n",
    "    global_reading_ease = textstat.flesch_reading_ease(content)\n",
    "\n",
    "    # Flesch Kincaid Grade Level\n",
    "    global_grade_level = textstat.flesch_kincaid_grade(content)\n",
    "\n",
    "    res = {\"headline\": inputData[\"Title\"],\n",
    "            \"content\": inputData[\"ArticleText\"],\n",
    "            \"n_tokens_title\": n_tokens_title, \n",
    "                                    \"title_subjectivity\": title_subjectivity,\n",
    "                                    \"title_sentiment_polarity\": title_sentiment_polarity,\n",
    "                                    \"title_sentiment_abs_polarity\": title_sentiment_abs_polarity,\n",
    "                                    \"average_token_length_title\": average_token_length_title,\n",
    "                                    \"n_tokens_content\": n_tokens_content,\n",
    "                                    \"r_unique_tokens\": r_unique_tokens,\n",
    "                                    \"r_non_stop_words\": r_non_stop_words,\n",
    "                                    \"r_non_stop_unique_tokens\": r_non_stop_unique_tokens,\n",
    "                                    \"average_token_length_content\": average_token_length_content,\n",
    "                                    \"global_subjectivity\": global_subjectivity,\n",
    "                                    \"global_sentiment_polarity\": global_sentiment_polarity,\n",
    "                                    \"global_sentiment_abs_polarity\": global_sentiment_abs_polarity,\n",
    "                                    \"global_rate_positive_words\": global_rate_positive_words,\n",
    "                                    \"global_rate_negative_words\": global_rate_negative_words,\n",
    "                                    \"rate_positive_words\": rate_positive_words,\n",
    "                                    \"rate_negative_words\": rate_negative_words,\n",
    "                                    \"avg_positive_polarity\": avg_positive_polarity,\n",
    "                                    \"min_positive_polarity\": min_positive_polarity,\n",
    "                                    \"max_positive_polarity\": max_positive_polarity,\n",
    "                                    \"avg_negative_polarity\": avg_negative_polarity,\n",
    "                                    \"min_negative_polarity\": min_negative_polarity,\n",
    "                                    \"max_negative_polarity\": max_negative_polarity,\n",
    "                                    \"max_abs_polarity\": max_abs_polarity,\n",
    "                                    \"global_reading_ease\": global_reading_ease,\n",
    "                                    \"global_grade_level\": global_grade_level}\n",
    "    \n",
    "    return(pd.DataFrame.from_dict(res, orient = 'index').transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "for index, row in inputData.head(142).iterrows():\n",
    "    \n",
    "    #print(row['ArticleText'])\n",
    "    \n",
    "    if index is 0:\n",
    "        res = engineer_NLP_features(row)\n",
    "    else:\n",
    "        res = res.append(engineer_NLP_features(row))\n",
    "\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('res_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LDA model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamulticore import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('File.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the reviews\n",
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopset = list(set(stopwords.words('english')))\n",
    "clean_reviews_text = []\n",
    "for review in df['ArticleText']:  # Loop through the tokens (the words or symbols) in each review. \n",
    "    try:    \n",
    "        cleaned_review = re.sub(\"[^a-zA-Z]\",\" \", review)  # Remove numbers and punctuation.\n",
    "        cleaned_review = cleaned_review.lower()  # Convert the text to lower case.\n",
    "        cleaned_review = ' '.join([word for word in cleaned_review.split() if word not in stopset])  # Keep only words that are not stopwords.\n",
    "        cleaned_review = ' '.join([wordnet_lemmatizer.lemmatize(word, pos='n') for word in cleaned_review.split()])  # Keep each noun's lemma.\n",
    "        cleaned_review = ' '.join([wordnet_lemmatizer.lemmatize(word, pos='v') for word in cleaned_review.split()])  # Keep each verb's lemma.\n",
    "        cleaned_review = re.sub(r\"(http\\S+)\",\" \", cleaned_review)  # Remove http links.\n",
    "        cleaned_review = ' '.join(cleaned_review.split())  # Remove white space.\n",
    "    except TypeError:\n",
    "        pass\n",
    "    clean_reviews_text.append(cleaned_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleanText'] = clean_reviews_text\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ORIGINAL: ' + df.iloc[0]['ArticleText'])\n",
    "print(' ')\n",
    "print('CLEANED: ' + df.iloc[0]['cleanText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleanText'].to_csv('cleanText.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform semantic analysis using LDA.\n",
    "Preprocess the reviews by creating a dictionary of words used and a bag-of-words corpus. Note that each of the steps below takes several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokens_by_doc = [review.split() for review in clean_reviews_text]\n",
    "dictionary = corpora.Dictionary(tokens_by_doc)\n",
    "bow_corpus = [dictionary.doc2bow(tokens) for tokens in tokens_by_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lda_model = LdaModel(bow_corpus, num_topics=10, id2word=dictionary, random_state=201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "topics_list = []\n",
    "for doc_topics in lda_model.get_document_topics(bow_corpus):\n",
    "    topics_list.append(sorted(doc_topics, key=lambda doc: -doc[1])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.get_document_topics(bow_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Top Topic'] = topics_list\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The plot on the left below is generated using multidimensional scaling, which is a general purpose algorithm for plotting, in two dimensions, items that are multidimensional (e.g., topics which are described by probabilities of thousands of words). For further reading on multidimensional scaling, see https://en.wikipedia.org/wiki/Multidimensional_scaling.\n",
    "\n",
    "When you hover over a topic on the left plot, you can then see a ranking of the most salient words on the right plot. Using these two plots, you might be able to create names for your ten topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topic(topicid=5, topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize your LDA results\n",
    "Preparing the visualization will take several minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mixture = [dict(lda_model[x]) for x in bow_corpus]\n",
    "pd.DataFrame(mixture).to_csv(\"topic_mixture86.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_per_topic = []\n",
    "for t in range(lda_model.num_topics):\n",
    "    top_words_per_topic.extend([(t, ) + x for x in lda_model.show_topic(t, topn = 5)])\n",
    "\n",
    "pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(\"top_words86.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
